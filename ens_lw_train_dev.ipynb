{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89aaa5b2-0ab5-451f-aa1b-66d6ff4bf444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T17:54:01.125323Z",
     "iopub.status.busy": "2023-05-18T17:54:01.125182Z",
     "iopub.status.idle": "2023-05-18T17:58:08.439198Z",
     "shell.execute_reply": "2023-05-18T17:58:08.438639Z",
     "shell.execute_reply.started": "2023-05-18T17:54:01.125308Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First run!\n",
      "Create experiment dir at: \n",
      " /tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY\n",
      "cp /tigress/cw55/work/2022_radi_nn/NN_AM4/NNRTMC_utils.py /tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/ \n",
      "copy trainscript failed\n",
      "Data files:\n",
      "['/scratch/gpfs/cw55/AM4/work/CTL2000_train_y2000_stellarcpu_intelmpi_22_768PE/HISTORY/20000101.atmos_8xdaily.tile1.nc', '/scratch/gpfs/cw55/AM4/work/CTL2000_train_y2000_stellarcpu_intelmpi_22_768PE/HISTORY/20000101.atmos_8xdaily.tile2.nc', '/scratch/gpfs/cw55/AM4/work/CTL2000_train_y2000_stellarcpu_intelmpi_22_768PE/HISTORY/20000101.atmos_8xdaily.tile3.nc', '/scratch/gpfs/cw55/AM4/work/CTL2000_train_y2000_stellarcpu_intelmpi_22_768PE/HISTORY/20000101.atmos_8xdaily.tile4.nc', '/scratch/gpfs/cw55/AM4/work/CTL2000_train_y2000_stellarcpu_intelmpi_22_768PE/HISTORY/20000101.atmos_8xdaily.tile5.nc', '/scratch/gpfs/cw55/AM4/work/CTL2000_train_y2000_stellarcpu_intelmpi_22_768PE/HISTORY/20000101.atmos_8xdaily.tile6.nc']\n",
      "Data selection:\n",
      "    Month: [1, 4, 7, 11] \n",
      "    Day: [1] \n",
      "Reading data... 0 1 2 3 4 5 Done.\n",
      "Features| Input: 101,  Output: 36\n",
      "Total data size: 1769472\n",
      "Total data size: 1769472\n",
      "Test data ratio: 0.3\n",
      "Train info >> run: 1 lr_sta: 1.0e-03, batch size: 8000\n",
      "Epoch 00001 |Train L: 1.38e-01 5.27e+01 | Vali. L: 1.5e-01 3.1e+01  | ~   1s | eta   0 min\n",
      "Epoch 00101 |Train L: 3.28e-03 1.57e+00 | Vali. L: 1.1e-02 1.4e+00  | ~  43s | eta   0 min\n",
      "Epoch 00192: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 00001 |Train L: 1.50e-01 2.79e+01 | Vali. L: 1.7e-01 4.2e+01  | ~   0s | eta   0 min\n",
      "Epoch 00101 |Train L: 3.36e-03 9.34e-01 | Vali. L: 1.0e-02 3.5e+00  | ~  42s | eta   0 min\n",
      "OUTPUT is saved at: /tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/model0_restart.01.pth\n",
      "error.shape\n",
      "torch.Size([530841, 36])\n",
      "TEST: RMSE, MAE, Bias\n",
      "tensor([2.1255, 2.5760, 1.3455, 0.2113, 0.1716, 0.1217, 0.0638, 0.0365, 0.0210,\n",
      "        0.0329, 0.0185, 0.0196, 0.0149, 0.0140, 0.0152, 0.0213, 0.0288, 0.0361,\n",
      "        0.0334, 0.0323, 0.0380, 0.0457, 0.0516, 0.0517, 0.0568, 0.0484, 0.0440,\n",
      "        0.0611, 0.0507, 0.0524, 0.0567, 0.0628, 0.0711, 0.1236, 0.1555, 0.3003]) \n",
      " tensor([1.6435, 1.7746, 0.8808, 0.0671, 0.0374, 0.0326, 0.0196, 0.0126, 0.0102,\n",
      "        0.0138, 0.0090, 0.0084, 0.0089, 0.0076, 0.0105, 0.0135, 0.0206, 0.0204,\n",
      "        0.0232, 0.0221, 0.0268, 0.0281, 0.0311, 0.0323, 0.0283, 0.0284, 0.0284,\n",
      "        0.0294, 0.0322, 0.0247, 0.0325, 0.0300, 0.0300, 0.0402, 0.0656, 0.1201]) \n",
      " tensor([-9.8866e-01, -6.7884e-01, -2.8368e-01,  3.5759e-02, -2.5940e-03,\n",
      "         2.7794e-02,  1.5608e-02,  1.5474e-03,  4.4044e-03,  6.8860e-03,\n",
      "         5.5354e-04, -3.4267e-03, -4.7471e-03, -5.4554e-03, -6.0026e-03,\n",
      "         3.2536e-04,  5.6847e-03, -2.6159e-03,  5.2297e-03,  2.4885e-03,\n",
      "         1.4185e-02, -1.4456e-02, -1.5092e-02, -5.0484e-03,  9.4880e-03,\n",
      "        -3.1877e-03,  1.6042e-02, -9.6541e-03,  1.6702e-02,  5.8201e-03,\n",
      "         9.3714e-03, -8.1018e-03, -3.6125e-03,  1.3123e-03, -7.8175e-03,\n",
      "        -8.0367e-02])\n",
      "OUTPUT is saved at: /tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/model1_restart.01.pth\n",
      "error.shape\n",
      "torch.Size([530841, 36])\n",
      "TEST: RMSE, MAE, Bias\n",
      "tensor([2.6735, 3.2687, 1.7306, 0.2592, 0.1503, 0.1273, 0.0723, 0.0442, 0.0346,\n",
      "        0.0238, 0.0287, 0.0276, 0.0185, 0.0117, 0.0210, 0.0517, 0.0695, 0.0659,\n",
      "        0.0518, 0.0478, 0.0517, 0.0559, 0.0810, 0.0754, 0.0602, 0.0726, 0.0690,\n",
      "        0.0880, 0.0570, 0.0998, 0.1109, 0.0905, 0.0940, 0.1140, 0.2298, 0.3582]) \n",
      " tensor([1.7505, 2.0498, 1.0453, 0.0948, 0.0458, 0.0308, 0.0262, 0.0218, 0.0212,\n",
      "        0.0151, 0.0181, 0.0213, 0.0128, 0.0082, 0.0169, 0.0416, 0.0589, 0.0545,\n",
      "        0.0402, 0.0345, 0.0362, 0.0392, 0.0636, 0.0584, 0.0430, 0.0502, 0.0500,\n",
      "        0.0664, 0.0375, 0.0773, 0.0886, 0.0479, 0.0510, 0.0605, 0.1332, 0.1845]) \n",
      " tensor([ 4.8319e-01, -2.4325e-01, -2.9933e-01,  7.4071e-02,  1.2544e-02,\n",
      "         6.2977e-03, -1.0925e-02, -1.2763e-02,  2.8467e-03,  5.6377e-03,\n",
      "        -8.0348e-03,  1.7643e-02,  1.8604e-04,  1.6998e-03, -1.5271e-02,\n",
      "        -3.8562e-02, -5.7127e-02,  4.8107e-02,  8.5634e-03, -9.1254e-03,\n",
      "        -1.4432e-02,  1.4511e-02, -4.6484e-02,  2.7455e-02,  2.9741e-02,\n",
      "         2.9617e-02,  1.9913e-03,  6.0786e-02,  2.1995e-03, -5.3500e-02,\n",
      "        -7.1917e-02,  1.0063e-02, -1.0845e-02, -1.8318e-02, -9.9929e-02,\n",
      "        -8.0418e-02])\n",
      "ens_AM4std_lw_cs_LiH4W256Relu_EY Finished: run 1!\n",
      "Train info >> run: 2 lr_sta: 1.0e-04, batch size: 32000\n",
      "Epoch 00001 |Train L: 7.75e-04 1.02e-01 | Vali. L: 5.0e-03 5.8e-01  | ~   0s | eta   0 min\n",
      "Epoch 00052: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Epoch 00101 |Train L: 7.65e-04 1.47e-01 | Vali. L: 5.2e-03 6.3e-01  | ~  17s | eta   0 min\n",
      "Epoch 00153: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 00001 |Train L: 9.35e-04 1.28e-01 | Vali. L: 4.9e-03 7.5e-01  | ~   0s | eta   0 min\n",
      "Epoch 00053: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Epoch 00101 |Train L: 7.12e-04 9.31e-02 | Vali. L: 4.7e-03 8.2e-01  | ~  17s | eta   0 min\n",
      "Epoch 00154: reducing learning rate of group 0 to 4.0000e-06.\n",
      "OUTPUT is saved at: /tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/model0_restart.02.pth\n",
      "error.shape\n",
      "torch.Size([530841, 36])\n",
      "TEST: RMSE, MAE, Bias\n",
      "tensor([1.4747, 1.9265, 1.0805, 0.2163, 0.1769, 0.1268, 0.0657, 0.0384, 0.0203,\n",
      "        0.0329, 0.0160, 0.0182, 0.0122, 0.0122, 0.0134, 0.0193, 0.0251, 0.0316,\n",
      "        0.0294, 0.0298, 0.0334, 0.0447, 0.0461, 0.0492, 0.0676, 0.0540, 0.0429,\n",
      "        0.0664, 0.0484, 0.0609, 0.0538, 0.0659, 0.0701, 0.1234, 0.1532, 0.2789]) \n",
      " tensor([0.9131, 1.1014, 0.6686, 0.0539, 0.0286, 0.0206, 0.0136, 0.0116, 0.0082,\n",
      "        0.0092, 0.0070, 0.0073, 0.0062, 0.0048, 0.0065, 0.0117, 0.0160, 0.0176,\n",
      "        0.0184, 0.0188, 0.0205, 0.0210, 0.0212, 0.0208, 0.0237, 0.0218, 0.0215,\n",
      "        0.0237, 0.0210, 0.0235, 0.0220, 0.0239, 0.0259, 0.0351, 0.0524, 0.0778]) \n",
      " tensor([ 1.1203e-01,  2.6810e-01,  8.0915e-02,  1.8781e-02,  1.4082e-02,\n",
      "         1.2259e-02,  7.1127e-03,  4.0912e-03, -2.2162e-04, -1.8504e-03,\n",
      "        -6.3125e-05, -8.7307e-04, -1.0949e-03, -1.6118e-03, -1.8947e-03,\n",
      "        -1.7845e-03, -2.6168e-03,  3.9147e-06,  2.7845e-03,  6.4542e-04,\n",
      "        -6.7103e-04,  1.0115e-03,  8.0827e-04,  1.1204e-03,  4.9227e-03,\n",
      "         4.4277e-03,  1.4845e-04,  8.1332e-03,  2.5643e-03,  7.4246e-03,\n",
      "         2.6872e-03,  4.7494e-03,  1.7409e-03,  5.4481e-03,  3.7229e-03,\n",
      "        -1.6509e-03])\n",
      "OUTPUT is saved at: /tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/model1_restart.02.pth\n",
      "error.shape\n",
      "torch.Size([530841, 36])\n",
      "TEST: RMSE, MAE, Bias\n",
      "tensor([2.0940, 2.6701, 1.3460, 0.2222, 0.1448, 0.1195, 0.0670, 0.0387, 0.0240,\n",
      "        0.0172, 0.0223, 0.0171, 0.0137, 0.0077, 0.0124, 0.0281, 0.0289, 0.0283,\n",
      "        0.0270, 0.0435, 0.0334, 0.0354, 0.0404, 0.0367, 0.0386, 0.0548, 0.0371,\n",
      "        0.0486, 0.0430, 0.0542, 0.0614, 0.0882, 0.0772, 0.0866, 0.1739, 0.2794]) \n",
      " tensor([1.0107, 1.1865, 0.6706, 0.0566, 0.0260, 0.0198, 0.0141, 0.0107, 0.0086,\n",
      "        0.0072, 0.0082, 0.0076, 0.0062, 0.0041, 0.0066, 0.0128, 0.0165, 0.0171,\n",
      "        0.0177, 0.0213, 0.0198, 0.0207, 0.0213, 0.0202, 0.0213, 0.0222, 0.0204,\n",
      "        0.0219, 0.0210, 0.0224, 0.0233, 0.0276, 0.0271, 0.0310, 0.0553, 0.0777]) \n",
      " tensor([ 1.7836e-01,  2.6752e-01,  6.3172e-02,  2.3047e-02,  1.1125e-02,\n",
      "         1.0847e-02,  6.1516e-03,  3.2770e-03, -1.7201e-04, -9.7503e-04,\n",
      "        -3.8015e-04,  8.7196e-04, -9.3909e-04, -2.1639e-04,  2.2190e-04,\n",
      "        -3.0683e-03, -2.8057e-03,  2.1468e-04,  2.7882e-04,  3.6957e-03,\n",
      "         3.1250e-03,  1.4548e-03, -9.4099e-04,  1.1745e-03, -6.4425e-05,\n",
      "        -1.8515e-03,  7.6781e-04,  2.3235e-03,  1.9773e-03,  1.1606e-05,\n",
      "        -1.2467e-03,  5.4110e-03,  1.5406e-03, -6.3876e-04, -5.3791e-03,\n",
      "        -2.1978e-03])\n",
      "ens_AM4std_lw_cs_LiH4W256Relu_EY Finished: run 2!\n",
      "All runs finished. Increase <run_num> if you need to continue to train the model.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn  \n",
    "import time \n",
    "import os  \n",
    "\n",
    "######################################################\n",
    "# common functions to split the training and test data\n",
    "# \n",
    "from NNRTMC_utils import NNRTMC_NN_lw, split_train_test_sample, \\\n",
    "draw_batches, data_std_normalization, print_key_results, return_exp_dir\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "######################################################\n",
    "def custom_trainning(NNRTMC_solver, lr, loss, epochs, batch_size, de_save,\n",
    "                     input_torch, output_torch,\n",
    "                     indice_train, indice_test, \n",
    "                     eng_loss_frac, device, rng):\n",
    "    # update lr based on test loss\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        NNRTMC_solver.optimizer, mode='min', factor=0.2, patience=50, threshold=1e-3, \n",
    "        threshold_mode='rel', cooldown=50, min_lr=0, eps=1e-08, verbose=True) \n",
    "    NNRTMC_solver.optimizer.param_groups[0]['lr'] = lr\n",
    "    ######################################################\n",
    "    # set training hyperparameter here\n",
    "    ######################################################\n",
    "    sta_time = time.time()\n",
    "    for t in range(epochs): \n",
    "        batch_indice_train = draw_batches(indice_train, batch_size, rng, device, replace=False)\n",
    "        lossv     = NNRTMC_solver.train(batch_indice_train, input_torch, output_torch, eng_loss_frac)\n",
    "        lossvtest = NNRTMC_solver.test_loss(indice_test, input_torch,  output_torch)\n",
    "        lr_scheduler.step(lossvtest[0]+lossvtest[1]) # update lr based on test loss\n",
    "        if t % de_save == 0:\n",
    "            used_time = time.time() - sta_time  \n",
    "            print( f\"Epoch {t+1:05d} |Train L: {lossv[0]:8.2e} {lossv[1]:8.2e} | Vali. L: {lossvtest[0]:7.1e} {lossvtest[1]:7.1e}  \"\n",
    "                  +f\"| ~ {used_time:3.0f}s | eta {int(used_time*((epochs-t)/de_save/60)) :3d} min\")\n",
    "            sta_time = time.time()\n",
    "            loss.append([[t+1]+lossv+lossvtest]) # append epochs, loss, test loss\n",
    "            # early stop \n",
    "            if NNRTMC_solver.optimizer.param_groups[0]['lr'] < 1e-7:\n",
    "                print(f\"Meet early stop criteria LR = {NNRTMC_solver.optimizer.param_groups[0]['lr']} < 1e-7\" )\n",
    "                print(\"End training\")\n",
    "                break\n",
    "    \n",
    "######################################################\n",
    "def custom_trainning_ens(NNRTMC_solver, lr, loss, epochs, batch_size, de_save,\n",
    "                         input_torch, output_torch,\n",
    "                         indice_train, indice_test, \n",
    "                         eng_loss_frac, device, rng):\n",
    "    for mi in range(len(NNRTMC_solver)): \n",
    "        loss_mi = []\n",
    "        custom_trainning(NNRTMC_solver[mi], lr_sta, loss_mi, epochs, batch_size, de_save,\\\n",
    "                         input_torch, output_torch, \\\n",
    "                         ind_train, ind_test, eng_loss_frac, device, rng )\n",
    "        loss.append(loss_mi)\n",
    "        \n",
    "######################################################\n",
    "import xarray as xr  \n",
    "from get_data_lw_AM4_std import get_data_lw_AM4\n",
    "import argparse, sys\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    torch.cuda.set_device(0) # select gpu_id, default 0 means the first GPU\n",
    "    device = f'cuda:{torch.cuda.current_device()}'\n",
    "    # set random generator\n",
    "    rng = np.random.default_rng(12345)\n",
    "    torch.manual_seed(12345)\n",
    "    # rng = np.random.default_rng()\n",
    "    \n",
    "    #####################################################\n",
    "    # set exp name and runs \n",
    "    # read sky_cond and eng_loss from terminal command\n",
    "    # parser=argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--sky_cond\", help=\"sky condition: af, csaf\")\n",
    "    # parser.add_argument(\"--eng_loss\", help=\"minimize the energy loss: Y/N\")\n",
    "    # args=parser.parse_args()\n",
    "    # sky_cond = args.sky_cond\n",
    "    # eng_loss = args.eng_loss  \n",
    "    sky_cond = 'cs'\n",
    "    # sky_cond = 'all'\n",
    "    eng_loss = 'Y'\n",
    "    \n",
    "    hidden_layer_width = 256\n",
    "    # hidden_layer_width = 32\n",
    "    ensemble_num = 2\n",
    "    \n",
    "    Exp_name = f'ens_AM4std_lw_{sky_cond}_LiH4W{hidden_layer_width}Relu_E{eng_loss}' \n",
    "    work_dir = '/tigress/cw55/work/2022_radi_nn/NN_AM4/work/'\n",
    "    total_run_num  = 2\n",
    "    epochs = 200\n",
    "    de_save = 100 \n",
    "    \n",
    "    if eng_loss != 'Y':\n",
    "        eng_loss_frac = None\n",
    "    else:\n",
    "        if sky_cond == 'cs':\n",
    "            eng_loss_frac = 1e-4 # lower loss weight for cs?\n",
    "        else:\n",
    "            eng_loss_frac = 1e-4\n",
    "        \n",
    "    ######################################################\n",
    "    # create dir for first run or load restart file\n",
    "    run_num, exp_dir = return_exp_dir(work_dir, Exp_name)\n",
    "    # copy script to experiment dir for reference\n",
    "    try:\n",
    "        ossyscmd = f'cp {os.path.abspath(__file__)} {exp_dir}/train_script{run_num:02d}.py' \n",
    "        os.system(ossyscmd) \n",
    "    except: \n",
    "        print('copy trainscript failed')\n",
    "    model_state_dict = []\n",
    "    # get restart info\n",
    "    if run_num == 1:  \n",
    "        nor_para = None\n",
    "        for mi in range(ensemble_num):\n",
    "            model_state_dict.append(None)\n",
    "        lr_sta = 1e-3\n",
    "    else:   # load restart file\n",
    "        for mi in range(ensemble_num):\n",
    "            PATH_last =  exp_dir+f'/model{mi}_restart.{run_num-1:02d}.pth'\n",
    "            restart_data = torch.load(PATH_last)  # load exist results and restart training\n",
    "            print(f'restart from: {PATH_last}')\n",
    "            # read training dataset, nor_para, model parameteres\n",
    "            nor_para = restart_data['nor_para']\n",
    "            model_state_dict.append(restart_data['model_state_dict'])\n",
    "        lr_sta = 1e-4\n",
    "\n",
    "    ######################################################\n",
    "    # load data from AM4 runs\n",
    "    filelist = [f'/scratch/gpfs/cw55/AM4/work/CTL2000_train_y2000_stellarcpu_intelmpi_22_768PE/'+\n",
    "            f'HISTORY/20000101.atmos_8xdaily.tile{_}.nc' for _ in range(1,7)] \n",
    "    # input_array_ori, output_array_ori = \\\n",
    "    # get_data_sw_AM4(filelist, condition=sky_cond, month_sel = None, day_sel = [1,7]) \n",
    "    input_array_ori, output_array_ori = \\\n",
    "    get_data_lw_AM4(filelist, condition=sky_cond, month_sel = [1, 4, 7, 11], day_sel = [1]) \n",
    "    \n",
    "    hybrid_p_sigma_para = xr.open_dataset('/tigress/cw55/data/NNRTMC_dataset/AM4_pk_bk_202207.nc')\n",
    "    A_k = hybrid_p_sigma_para.ak.values[None,:]\n",
    "    B_k = hybrid_p_sigma_para.bk.values[None,:]\n",
    "    \n",
    "    print(f\"Features| Input: {input_array_ori.shape[1]},  Output: {output_array_ori.shape[1]}\")\n",
    "    nor_para, input_array, output_array = \\\n",
    "    data_std_normalization(input_array_ori, output_array_ori, nor_para)  \n",
    " \n",
    "\n",
    "    # divide the training and test data here\n",
    "    # this would be different if restart the training process (be careful!)\n",
    "    ind_train, ind_test = split_train_test_sample(output_array.shape[0], test_ratio=0.3, rng=rng) \n",
    "    \n",
    "    ######################################################\n",
    "    # move all data to GPU to accelerate training\n",
    "    input_torch  = torch.tensor(input_array,  dtype=torch.float32).to(device)\n",
    "    output_torch = torch.tensor(output_array, dtype=torch.float32).to(device) \n",
    "    \n",
    "    ######################################################\n",
    "    # initialize model\n",
    "    NNRTMC_solver = []\n",
    "    for mi in range(ensemble_num):\n",
    "        NNRTMC_solver.append(NNRTMC_NN_lw(device, nor_para, A_k, B_k, \n",
    "                             input_array.shape[1], hidden_layer_width, model_state_dict[mi]))\n",
    "    # training \n",
    "    for i in range(run_num, total_run_num+1): \n",
    "        loss = []\n",
    "        batch_size = max(8000, 8000*i**2)\n",
    "        print(f'Train info >> run: {i} lr_sta: {lr_sta:7.1e}, batch size: {batch_size}')\n",
    "        custom_trainning_ens(NNRTMC_solver, lr_sta, loss, epochs, batch_size, de_save,\\\n",
    "                             input_torch, output_torch, \\\n",
    "                             ind_train, ind_test, eng_loss_frac, device, rng )\n",
    "        ######################################################\n",
    "        # save model state dict and data normalization info\n",
    "        data_info = filelist\n",
    "        for mi in range(len(NNRTMC_solver)):\n",
    "            loss_array = np.array(loss[mi]).squeeze().T  \n",
    "            PATH =  exp_dir+f'/model{mi}_restart.{i:02d}.pth'\n",
    "            NNRTMC_solver[mi].save_model_restart(PATH, loss_array, data_info, nor_para)\n",
    "            print('OUTPUT is saved at: '+PATH)        \n",
    "            print_key_results(NNRTMC_solver[mi].predict(input_torch[ind_test,:]), \n",
    "                              output_array[ind_test,:], \n",
    "                              nor_para)\n",
    "        lr_sta = 1e-4\n",
    "        print(f'{Exp_name} Finished: run {i}!')  \n",
    "        \n",
    "    print('All runs finished. Increase <run_num> if you need to continue to train the model.')\n",
    "\n",
    "    # # move slurm log to work dir\n",
    "    # job_id = int(os.environ[\"SLURM_JOB_ID\"])\n",
    "    # ossyscmd = f'cp slurm-{job_id}.out {exp_dir}/' \n",
    "    # print(ossyscmd)\n",
    "    # os.system(ossyscmd) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b84a1-0ef1-48b0-a18d-33f5050be3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673db8d1-3704-4fe3-90ca-e6b699015f39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T17:58:08.440774Z",
     "iopub.status.busy": "2023-05-18T17:58:08.440395Z",
     "iopub.status.idle": "2023-05-18T17:58:08.449767Z",
     "shell.execute_reply": "2023-05-18T17:58:08.449343Z",
     "shell.execute_reply.started": "2023-05-18T17:58:08.440757Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3953be0-a1d8-4c06-b867-eaf0db32db75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T17:58:08.450278Z",
     "iopub.status.busy": "2023-05-18T17:58:08.450148Z",
     "iopub.status.idle": "2023-05-18T17:58:08.458130Z",
     "shell.execute_reply": "2023-05-18T17:58:08.457514Z",
     "shell.execute_reply.started": "2023-05-18T17:58:08.450266Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1,\n",
       "   0.0009347588056698442,\n",
       "   0.12828442454338074,\n",
       "   0.004868125077337027,\n",
       "   0.7548418641090393]],\n",
       " [[101,\n",
       "   0.0007121806847862899,\n",
       "   0.09308461844921112,\n",
       "   0.004670975264161825,\n",
       "   0.8179610371589661]]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f618aeae-1758-49c3-8fd5-1fc929ee3611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T17:58:08.458616Z",
     "iopub.status.busy": "2023-05-18T17:58:08.458488Z",
     "iopub.status.idle": "2023-05-18T17:58:08.466016Z",
     "shell.execute_reply": "2023-05-18T17:58:08.465582Z",
     "shell.execute_reply.started": "2023-05-18T17:58:08.458603Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "127a013e-f579-4876-a1fc-ddc94d780781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T17:58:08.466961Z",
     "iopub.status.busy": "2023-05-18T17:58:08.466814Z",
     "iopub.status.idle": "2023-05-18T17:58:08.474403Z",
     "shell.execute_reply": "2023-05-18T17:58:08.474002Z",
     "shell.execute_reply.started": "2023-05-18T17:58:08.466948Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/tigress/cw55/work/2022_radi_nn/NN_AM4/work/',\n",
       " 'ens_AM4std_lw_cs_LiH4W256Relu_EY')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_dir,Exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14025229-1dc6-4203-8559-38a33e9a6a20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T17:58:08.474893Z",
     "iopub.status.busy": "2023-05-18T17:58:08.474754Z",
     "iopub.status.idle": "2023-05-18T17:58:08.625774Z",
     "shell.execute_reply": "2023-05-18T17:58:08.625272Z",
     "shell.execute_reply.started": "2023-05-18T17:58:08.474878Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/model0_restart.01.pth\n",
      "/tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/model0_restart.02.pth\n",
      "/tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/model1_restart.01.pth\n",
      "/tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/model1_restart.02.pth\n",
      "/tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/NNRTMC_utils.py\n"
     ]
    }
   ],
   "source": [
    "ls /tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5e94c98-9a22-42c7-850d-e4d752cd823a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-18T17:58:08.628298Z",
     "iopub.status.busy": "2023-05-18T17:58:08.626543Z",
     "iopub.status.idle": "2023-05-18T17:58:08.773447Z",
     "shell.execute_reply": "2023-05-18T17:58:08.772860Z",
     "shell.execute_reply.started": "2023-05-18T17:58:08.628278Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rm -rf /tigress/cw55/work/2022_radi_nn/NN_AM4/work/ens_AM4std_lw_cs_LiH4W256Relu_EY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba672f57-1163-49ac-ac3b-03877fe263b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch20",
   "language": "python",
   "name": "pytorch20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
