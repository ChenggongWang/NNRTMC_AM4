{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "# define ANN structure\n",
    "class ResNN_3L_block(nn.Module):\n",
    "    def __init__(self,input_features, hidden_layer_nodes):\n",
    "        super(ResNN_3L_block, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.ln1 = nn.Linear(input_features, hidden_layer_nodes)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_layer_nodes) \n",
    "        self.relu1  = nn.ReLU()\n",
    "        self.ln2 = nn.Linear(hidden_layer_nodes, hidden_layer_nodes)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_layer_nodes)   \n",
    "        self.relu2  = nn.ReLU()\n",
    "        self.ln3 = nn.Linear(hidden_layer_nodes, hidden_layer_nodes)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_layer_nodes)  \n",
    "        self.relu3  = nn.ReLU() \n",
    "        \n",
    "    def forward(self, x):\n",
    "        Y = self.relu1(self.bn1(self.ln1(x)))  # 1st hidden layer\n",
    "        \n",
    "        Y = self.relu2(self.bn2(self.ln2(Y)))  # 2nd hidden layer\n",
    "        \n",
    "        Y = self.relu3(self.bn3(self.ln3(Y)))  # 3rd hidden layer\n",
    "        \n",
    "        # skip connection for resnet\n",
    "        Y = Y+nn.functional.pad(x, (0, Y.shape[-1]-x.shape[-1]), \"constant\", 0) \n",
    "        \n",
    "        return Y         # output layer\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        input_feature_num  = 102\n",
    "        hidden_layer_width = 256\n",
    "        output_feature_num = 36\n",
    "        self.Res_stack = nn.Sequential(\n",
    "            nn.Linear(input_feature_num, hidden_layer_width),\n",
    "            nn.BatchNorm1d(hidden_layer_width) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_width, hidden_layer_width),\n",
    "            nn.BatchNorm1d(hidden_layer_width) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_width, hidden_layer_width),\n",
    "            nn.BatchNorm1d(hidden_layer_width) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_width, hidden_layer_width),\n",
    "            nn.BatchNorm1d(hidden_layer_width) ,\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_layer_width, output_feature_num)\n",
    "        ) \n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.Res_stack(x)\n",
    "\n",
    "    \n",
    "class ResNNRTMC: \n",
    "    def __init__(self,  device, nor_para, Ak, Bk, model_dict = None):\n",
    "        \"\"\"\n",
    "        model_dict_path is the saved NN model state dict.\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        # initial ANN\n",
    "        self.NN_model = NeuralNetwork().to(self.device) \n",
    "        if model_dict is not None:\n",
    "            self.NN_model.load_state_dict(model_dict) \n",
    "        self.optimizer = torch.optim.Adam(self.NN_model.parameters(), lr=1e-4, \n",
    "                                          betas=(0.9, 0.999), weight_decay=1e-4) \n",
    "        self.loss_fn   = torch.nn.MSELoss()\n",
    "        self.nor_para  = {key: torch.tensor(value).to(self.device) for key, value in nor_para.items()} \n",
    "        \n",
    "        # parameters\n",
    "        self.Ak = torch.tensor(Ak).to(self.device)\n",
    "        self.Bk = torch.tensor(Bk).to(self.device)\n",
    "        \n",
    "        self.C_p = 1004       # J/kg/K \n",
    "        self.g   = 9.8        # m/s^2 \n",
    "        \n",
    "    def train(self, train_batch_indice, input_torch, output_torch, optimizer=None):\n",
    "        if optimizer is None: \n",
    "            optimizer = self.optimizer\n",
    "        self.NN_model.train() # enter training mode\n",
    "        for i, batch_ids in enumerate(train_batch_indice):\n",
    "            X, Y = input_torch[batch_ids,:], output_torch[batch_ids,:]\n",
    "            # Compute prediction error\n",
    "            Y_pred = self.NN_model(X) \n",
    "            loss_data = self.loss_fn(Y_pred, Y)\n",
    "            loss_ener = self.loss_energy(X, Y_pred)\n",
    "            loss = loss_data\n",
    "            # loss += loss_ener*1e-3\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return [loss_data.item(), loss_ener.item()]\n",
    "    \n",
    "\n",
    "    def test_loss(self, test_indice, input_torch, output_torch):\n",
    "        self.NN_model.eval() # enter evaluation mode\n",
    "        with torch.no_grad():\n",
    "            X, Y = input_torch [test_indice,:], output_torch[test_indice,:]\n",
    "            Y_pred = self.NN_model(X) \n",
    "            loss_data = self.loss_fn(Y_pred, Y).item() \n",
    "            loss_ener = self.loss_energy(X, Y_pred).item() \n",
    "        return [loss_data, loss_ener]\n",
    " \n",
    "\n",
    "    def loss_energy(self,  input_data, output_pred ): \n",
    "        F_net, sum_Cphr_gdp = self.energy_flux_HR(input_data , output_pred) \n",
    "        return self.loss_fn(F_net,sum_Cphr_gdp) \n",
    "\n",
    "    def energy_flux_HR(self, Input, Output):  \n",
    "        F_toa_up = Output[:,2]/self.nor_para['output_scale'][2] + self.nor_para['output_offset'][2]\n",
    "        F_sfc_do = Output[:,0]/self.nor_para['output_scale'][0] + self.nor_para['output_offset'][0]\n",
    "        F_sfc_up = Output[:,1]/self.nor_para['output_scale'][1] + self.nor_para['output_offset'][1]\n",
    "        F_net = F_sfc_up - F_sfc_do - F_toa_up\n",
    "        HR = Output[:,3:]/self.nor_para['output_scale'][3:] + self.nor_para['output_offset'][3:]            #  K/s \n",
    "        ps = Input[:,None,0]/self.nor_para['input_scale'][0] + self.nor_para['input_offset'][0]\n",
    "        P_lev = self.return_dP_AM4_plev(ps)                #  Pa \n",
    "        dP = (P_lev[:,1:] - P_lev[:,:33])\n",
    "        sum_Cphr_gdp = self.C_p/self.g * (HR*dP).sum(axis=-1) \n",
    "        return F_net, sum_Cphr_gdp\n",
    "    \n",
    "    def predict(self, input_X):\n",
    "        self.NN_model.eval() # enter evaluation mode\n",
    "        with torch.no_grad():\n",
    "            pred = self.NN_model(input_X).cpu() # analyize on cpu\n",
    "        return pred\n",
    " \n",
    "    def return_dP_AM4_plev(self, ps): \n",
    "        \"\"\"\n",
    "        ps: Pa\n",
    "\n",
    "        return: \n",
    "        \"\"\" \n",
    "        p_int = self.Ak + self.Bk*ps\n",
    "        # if not np.all(np.diff(p_int)>0):\n",
    "        #     raise Exception(f'Input [ps] is not valid. Check units!')\n",
    "        return p_int\n",
    "    \n",
    "    def save_model_restart(self, PATH, loss_array, YMD_info, nomral_para):\n",
    "        '''\n",
    "        Saving & Loading Model for Inference\n",
    "        Save/Load state_dict (Recommended)\n",
    "        Load: \n",
    "        NNRTMC_solver= NNRTMC(device, model_dict_path = model_state_dict)\n",
    "        '''\n",
    "        # get a cpu copy of state dict\n",
    "        model_state_dict = {key: value.cpu() for key, value in self.NN_model.state_dict().items()}\n",
    "        # Save:\n",
    "        torch.save({ 'data_date_info'  : YMD_info,\n",
    "                     'loss_array'      : loss_array,\n",
    "                     'nor_para'        : nomral_para,\n",
    "                     'model_state_dict': model_state_dict\n",
    "                   } , PATH) \n",
    "\n",
    "import numpy as np  \n",
    "import time \n",
    "import os \n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "\n",
    "######################################################\n",
    "# NN module includes:\n",
    "# NN model structure \n",
    "# common functions to split the training and test data\n",
    "# \n",
    "from NNRTMC_lw_utils import  split_train_test_sample, \\\n",
    "draw_batches, data_std_normalization, print_key_results, return_exp_dir\n",
    "\n",
    "    \n",
    "######################################################\n",
    "def custom_trainning(NNRTMC_solver, lr, loss, epochs, batch_size, de_save,\n",
    "                     input_torch, output_torch, indice_train, indice_test, \n",
    "                     device, rng):\n",
    "    # update lr based on test loss\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        NNRTMC_solver.optimizer, mode='min', factor=0.2, patience=1000, threshold=1e-3, \n",
    "        threshold_mode='rel', cooldown=500, min_lr=0, eps=1e-08, verbose=True) \n",
    "    NNRTMC_solver.optimizer.param_groups[0]['lr'] = lr\n",
    "    ######################################################\n",
    "    # set training hyperparameter here\n",
    "    ######################################################\n",
    "    sta_time = time.time()\n",
    "    for t in range(epochs): \n",
    "        batch_indice_train = draw_batches(indice_train, batch_size, rng, device, replace=False)\n",
    "        lossv     = NNRTMC_solver.train(batch_indice_train, input_torch, output_torch)\n",
    "        lossvtest = NNRTMC_solver.test_loss(indice_test, input_torch,  output_torch)\n",
    "        lr_scheduler.step(lossvtest[0]+lossvtest[1]) # update lr based on test loss\n",
    "        if t % de_save == 0:\n",
    "            used_time = time.time() - sta_time  \n",
    "            print( f\"Epoch {t+1:06d} |train loss: {lossv[0]:8.2e} {lossv[1]:8.2e} | test loss: {lossvtest[0]:8.2e} {lossvtest[1]:8.2e}  \"\n",
    "                  +f\"|  use {used_time:3.0f}s | eta {int(used_time*((epochs-t)/de_save/60)) :3d} min\")\n",
    "            sta_time = time.time()\n",
    "            loss.append([[t+1]+lossv+lossvtest]) # append epochs, loss, test loss\n",
    "            # early stop \n",
    "            if NNRTMC_solver.optimizer.param_groups[0]['lr'] < 1e-7:\n",
    "                print(f\"Meet early stop criteria LR = {NNRTMC_solver.optimizer.param_groups[0]['lr']} < 1e-7\" )\n",
    "                print(\"End training\")\n",
    "                break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 5 input feature(s) is fixed!\n",
      "OUTPUT will be saved at: /tigress/cw55/work/2022_radi_nn/NN_lw_test/AM4_v0/afclr_L5_EN_A100/restart.00.pth\n",
      "Train info >> run: 0 lr_sta: 1.0e-03, batch size: 2000\n",
      "Epoch 000001 |train loss: 1.71e-01 6.21e+01 | test loss: 2.18e-01 5.86e+01  |  use   3s | eta   0 min\n",
      "Epoch 000501 |train loss: 8.06e-03 2.25e+00 | test loss: 1.02e-02 2.79e+00  |  use  59s | eta   8 min\n",
      "Epoch 001001 |train loss: 4.94e-03 1.45e+00 | test loss: 8.25e-03 1.79e+00  |  use  59s | eta   7 min\n",
      "Epoch 001501 |train loss: 7.37e-03 9.47e-01 | test loss: 8.37e-03 1.14e+00  |  use  59s | eta   6 min\n",
      "Epoch 002001 |train loss: 4.06e-03 1.10e+00 | test loss: 8.07e-03 1.65e+00  |  use  59s | eta   5 min\n",
      "Epoch 002501 |train loss: 4.29e-03 1.08e+00 | test loss: 7.24e-03 2.97e+00  |  use  59s | eta   4 min\n",
      "Epoch 003001 |train loss: 5.08e-03 1.02e+00 | test loss: 8.92e-03 8.83e-01  |  use  59s | eta   3 min\n",
      "Epoch 003501 |train loss: 3.78e-03 1.11e+00 | test loss: 9.36e-03 1.20e+00  |  use  59s | eta   2 min\n",
      "Epoch 03718: reducing learning rate of group 0 to 2.0000e-04.\n",
      "Epoch 004001 |train loss: 1.95e-03 6.53e-01 | test loss: 2.29e-03 6.87e-01  |  use  59s | eta   1 min\n",
      "Epoch 004501 |train loss: 4.15e-03 6.70e-01 | test loss: 2.69e-03 7.72e-01  |  use  59s | eta   0 min\n",
      "afclr_L5_EN_A100 Finished: run 1!\n",
      "TEST: RMSE, MAE, Bias\n",
      "tensor([1.1095, 1.0342, 0.4624, 0.0321, 0.0169, 0.0114, 0.0142, 0.0054, 0.0063,\n",
      "        0.0054, 0.0061, 0.0071, 0.0064, 0.0050, 0.0094, 0.0144, 0.0288, 0.0286,\n",
      "        0.0440, 0.0322, 0.0393, 0.0508, 0.0441, 0.0512, 0.0471, 0.0578, 0.0469,\n",
      "        0.0544, 0.0581, 0.0530, 0.0555, 0.0573, 0.0739, 0.0877, 0.1296, 0.2427]) tensor([0.8806, 0.8527, 0.3505, 0.0257, 0.0136, 0.0088, 0.0123, 0.0041, 0.0050,\n",
      "        0.0043, 0.0048, 0.0053, 0.0053, 0.0040, 0.0077, 0.0110, 0.0234, 0.0225,\n",
      "        0.0363, 0.0245, 0.0291, 0.0373, 0.0327, 0.0402, 0.0348, 0.0473, 0.0331,\n",
      "        0.0348, 0.0390, 0.0319, 0.0375, 0.0356, 0.0440, 0.0505, 0.0792, 0.1793]) tensor([-5.8471e-01, -6.5099e-01,  2.0037e-02, -1.0443e-02,  2.4631e-04,\n",
      "         1.1365e-03, -1.2049e-02,  1.6010e-04, -3.0795e-03, -2.8523e-03,\n",
      "        -1.2871e-03,  3.4204e-03,  4.7744e-03,  3.2979e-04,  5.1315e-03,\n",
      "         1.1465e-03, -2.0731e-02, -2.8599e-03, -3.0259e-02,  4.4448e-04,\n",
      "         1.6531e-03, -3.5017e-03,  2.2706e-02,  3.0355e-02, -7.1415e-03,\n",
      "         3.7589e-02,  8.5850e-04, -1.6622e-02, -5.3060e-03,  1.0055e-02,\n",
      "         4.9417e-05, -5.6717e-03,  1.3750e-02, -2.1107e-02,  4.6298e-04,\n",
      "        -8.7320e-02])\n",
      "OUTPUT will be saved at: /tigress/cw55/work/2022_radi_nn/NN_lw_test/AM4_v0/afclr_L5_EN_A100/restart.01.pth\n",
      "Train info >> run: 1 lr_sta: 1.0e-04, batch size: 4000\n",
      "Epoch 000001 |train loss: 1.88e-03 1.01e+00 | test loss: 1.66e-03 5.56e-01  |  use   0s | eta   0 min\n",
      "Epoch 000501 |train loss: 2.19e-03 5.22e-01 | test loss: 1.86e-03 5.23e-01  |  use  31s | eta   4 min\n",
      "Epoch 001001 |train loss: 1.75e-03 5.92e-01 | test loss: 1.76e-03 4.80e-01  |  use  31s | eta   4 min\n",
      "Epoch 001501 |train loss: 2.79e-03 6.78e-01 | test loss: 1.76e-03 5.17e-01  |  use  31s | eta   3 min\n",
      "Epoch 01823: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Epoch 002001 |train loss: 1.44e-03 4.96e-01 | test loss: 1.21e-03 4.96e-01  |  use  31s | eta   3 min\n",
      "Epoch 002501 |train loss: 1.40e-03 4.81e-01 | test loss: 1.23e-03 4.71e-01  |  use  31s | eta   2 min\n",
      "Epoch 003001 |train loss: 1.68e-03 4.51e-01 | test loss: 1.21e-03 4.85e-01  |  use  31s | eta   2 min\n",
      "Epoch 03324: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 003501 |train loss: 1.60e-03 6.04e-01 | test loss: 1.10e-03 5.00e-01  |  use  31s | eta   1 min\n",
      "Epoch 004001 |train loss: 2.18e-03 5.63e-01 | test loss: 1.11e-03 4.96e-01  |  use  31s | eta   1 min\n",
      "Epoch 004501 |train loss: 1.71e-03 4.54e-01 | test loss: 1.10e-03 4.90e-01  |  use  31s | eta   0 min\n",
      "Epoch 04825: reducing learning rate of group 0 to 8.0000e-07.\n",
      "afclr_L5_EN_A100 Finished: run 2!\n",
      "TEST: RMSE, MAE, Bias\n",
      "tensor([0.6889, 0.6405, 0.3652, 0.0163, 0.0085, 0.0078, 0.0053, 0.0041, 0.0039,\n",
      "        0.0033, 0.0037, 0.0042, 0.0035, 0.0027, 0.0052, 0.0103, 0.0145, 0.0173,\n",
      "        0.0203, 0.0227, 0.0282, 0.0270, 0.0263, 0.0280, 0.0316, 0.0292, 0.0309,\n",
      "        0.0419, 0.0396, 0.0403, 0.0406, 0.0466, 0.0550, 0.0709, 0.1040, 0.1644]) tensor([0.5013, 0.4802, 0.2688, 0.0116, 0.0053, 0.0055, 0.0037, 0.0030, 0.0027,\n",
      "        0.0023, 0.0027, 0.0031, 0.0025, 0.0019, 0.0037, 0.0077, 0.0106, 0.0128,\n",
      "        0.0145, 0.0164, 0.0203, 0.0188, 0.0182, 0.0190, 0.0208, 0.0190, 0.0186,\n",
      "        0.0218, 0.0217, 0.0216, 0.0222, 0.0240, 0.0278, 0.0345, 0.0519, 0.0993]) tensor([-5.9973e-02, -1.4653e-01, -2.0152e-02, -8.1055e-04,  3.2550e-05,\n",
      "        -2.1708e-03, -9.8809e-04, -1.5821e-03, -1.2100e-03, -2.3897e-04,\n",
      "        -1.6577e-04, -3.6350e-05,  2.2122e-04,  2.2107e-04,  5.8037e-04,\n",
      "         1.5023e-04, -8.1628e-04, -9.4578e-04, -9.8713e-04,  1.0409e-03,\n",
      "         8.4251e-04, -1.6219e-03, -9.3504e-04, -2.2883e-03, -1.3662e-03,\n",
      "        -7.9525e-04, -4.3829e-04,  5.7618e-04,  6.1760e-04,  2.8402e-05,\n",
      "        -1.2196e-03, -1.3337e-03, -7.8429e-04, -2.2038e-03, -7.2510e-03,\n",
      "        -1.2954e-02])\n",
      "OUTPUT will be saved at: /tigress/cw55/work/2022_radi_nn/NN_lw_test/AM4_v0/afclr_L5_EN_A100/restart.02.pth\n",
      "Train info >> run: 2 lr_sta: 1.0e-04, batch size: 16000\n",
      "Epoch 000001 |train loss: 1.07e-02 7.60e-01 | test loss: 1.94e-02 3.39e+00  |  use   0s | eta   0 min\n",
      "Epoch 000501 |train loss: 1.02e-03 6.65e-01 | test loss: 1.36e-03 4.84e-01  |  use  11s | eta   1 min\n",
      "Epoch 001001 |train loss: 1.01e-03 4.50e-01 | test loss: 1.33e-03 4.55e-01  |  use  11s | eta   1 min\n",
      "Epoch 001501 |train loss: 8.75e-04 5.10e-01 | test loss: 1.46e-03 4.93e-01  |  use  11s | eta   1 min\n",
      "Epoch 002001 |train loss: 8.18e-04 4.28e-01 | test loss: 1.32e-03 4.71e-01  |  use  11s | eta   1 min\n",
      "Epoch 002501 |train loss: 8.68e-04 4.98e-01 | test loss: 1.32e-03 5.14e-01  |  use  11s | eta   0 min\n",
      "Epoch 003001 |train loss: 9.57e-04 3.93e-01 | test loss: 1.47e-03 4.76e-01  |  use  11s | eta   0 min\n",
      "Epoch 03269: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Epoch 003501 |train loss: 8.18e-04 3.91e-01 | test loss: 1.08e-03 4.74e-01  |  use  11s | eta   0 min\n",
      "Epoch 004001 |train loss: 8.28e-04 4.67e-01 | test loss: 1.07e-03 4.77e-01  |  use  11s | eta   0 min\n",
      "Epoch 004501 |train loss: 8.81e-04 4.59e-01 | test loss: 1.09e-03 4.83e-01  |  use  11s | eta   0 min\n",
      "Epoch 04770: reducing learning rate of group 0 to 4.0000e-06.\n",
      "afclr_L5_EN_A100 Finished: run 3!\n",
      "TEST: RMSE, MAE, Bias\n",
      "tensor([0.6476, 0.5920, 0.3464, 0.0157, 0.0082, 0.0071, 0.0050, 0.0035, 0.0037,\n",
      "        0.0033, 0.0036, 0.0040, 0.0033, 0.0027, 0.0051, 0.0100, 0.0140, 0.0168,\n",
      "        0.0194, 0.0219, 0.0269, 0.0261, 0.0256, 0.0272, 0.0302, 0.0278, 0.0293,\n",
      "        0.0394, 0.0378, 0.0392, 0.0401, 0.0467, 0.0548, 0.0714, 0.1025, 0.1605]) tensor([0.4611, 0.4418, 0.2531, 0.0111, 0.0051, 0.0048, 0.0033, 0.0025, 0.0023,\n",
      "        0.0022, 0.0026, 0.0030, 0.0024, 0.0019, 0.0036, 0.0074, 0.0102, 0.0124,\n",
      "        0.0138, 0.0158, 0.0194, 0.0181, 0.0177, 0.0184, 0.0201, 0.0182, 0.0179,\n",
      "        0.0211, 0.0211, 0.0213, 0.0216, 0.0235, 0.0273, 0.0339, 0.0502, 0.0941]) tensor([-4.8229e-03, -2.9708e-02,  6.7377e-03, -7.3150e-04, -1.7704e-04,\n",
      "         7.5191e-04, -1.8633e-04,  3.0705e-05, -8.6844e-05, -1.7391e-04,\n",
      "         8.3069e-05,  4.5926e-04,  2.0903e-04, -2.5485e-04, -7.2954e-04,\n",
      "        -5.7577e-04,  3.5782e-04, -7.4301e-04, -4.9569e-04, -2.0604e-03,\n",
      "         1.2042e-03,  1.3982e-03,  5.5973e-04, -2.0670e-03, -2.0443e-03,\n",
      "        -6.0203e-04,  1.8181e-05,  1.5813e-03,  1.4563e-03,  1.7474e-03,\n",
      "        -4.2140e-04,  1.0906e-03,  1.0804e-03,  2.3482e-03,  7.4888e-04,\n",
      "         2.5497e-04])\n",
      "OUTPUT will be saved at: /tigress/cw55/work/2022_radi_nn/NN_lw_test/AM4_v0/afclr_L5_EN_A100/restart.03.pth\n",
      "Train info >> run: 3 lr_sta: 1.0e-04, batch size: 36000\n",
      "Epoch 000001 |train loss: 8.19e-04 4.16e-01 | test loss: 5.64e-03 4.26e-01  |  use   0s | eta   0 min\n",
      "Epoch 000501 |train loss: 5.55e-04 4.13e-01 | test loss: 1.14e-03 4.93e-01  |  use   8s | eta   1 min\n",
      "Epoch 001001 |train loss: 5.81e-04 4.40e-01 | test loss: 1.10e-03 4.72e-01  |  use   8s | eta   1 min\n",
      "Epoch 01002: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Epoch 001501 |train loss: 5.10e-04 4.27e-01 | test loss: 1.01e-03 4.62e-01  |  use   8s | eta   0 min\n",
      "Epoch 002001 |train loss: 4.70e-04 3.94e-01 | test loss: 1.01e-03 4.64e-01  |  use   8s | eta   0 min\n",
      "Epoch 002501 |train loss: 5.00e-04 4.14e-01 | test loss: 1.01e-03 4.64e-01  |  use   8s | eta   0 min\n",
      "Epoch 02503: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 003001 |train loss: 4.59e-04 4.39e-01 | test loss: 1.00e-03 4.66e-01  |  use   8s | eta   0 min\n",
      "Epoch 003501 |train loss: 4.71e-04 4.31e-01 | test loss: 1.00e-03 4.64e-01  |  use   8s | eta   0 min\n",
      "Epoch 004001 |train loss: 4.55e-04 4.01e-01 | test loss: 1.00e-03 4.64e-01  |  use   8s | eta   0 min\n",
      "Epoch 04004: reducing learning rate of group 0 to 8.0000e-07.\n",
      "Epoch 004501 |train loss: 4.97e-04 4.24e-01 | test loss: 1.00e-03 4.63e-01  |  use   8s | eta   0 min\n",
      "afclr_L5_EN_A100 Finished: run 4!\n",
      "TEST: RMSE, MAE, Bias\n",
      "tensor([0.6364, 0.5859, 0.3443, 0.0157, 0.0081, 0.0071, 0.0051, 0.0035, 0.0037,\n",
      "        0.0032, 0.0036, 0.0040, 0.0032, 0.0026, 0.0049, 0.0100, 0.0141, 0.0167,\n",
      "        0.0191, 0.0216, 0.0265, 0.0257, 0.0251, 0.0268, 0.0295, 0.0271, 0.0282,\n",
      "        0.0379, 0.0370, 0.0383, 0.0401, 0.0463, 0.0546, 0.0714, 0.1021, 0.1587]) tensor([0.4540, 0.4378, 0.2510, 0.0110, 0.0049, 0.0047, 0.0033, 0.0024, 0.0023,\n",
      "        0.0022, 0.0026, 0.0029, 0.0023, 0.0018, 0.0035, 0.0074, 0.0102, 0.0123,\n",
      "        0.0136, 0.0157, 0.0191, 0.0178, 0.0175, 0.0181, 0.0198, 0.0179, 0.0175,\n",
      "        0.0204, 0.0207, 0.0208, 0.0214, 0.0232, 0.0270, 0.0336, 0.0495, 0.0928]) tensor([ 1.3659e-02, -1.3282e-02,  1.9699e-02, -1.0989e-03,  4.2340e-04,\n",
      "         1.6537e-05, -1.7331e-04, -2.2048e-06, -4.7637e-05, -3.4192e-05,\n",
      "         5.8665e-05,  3.0387e-04,  6.5504e-05,  1.1156e-05, -1.4210e-04,\n",
      "        -2.5271e-04, -1.3949e-04, -6.8369e-04, -5.6717e-04, -9.8233e-05,\n",
      "         1.7277e-04, -7.8904e-04,  5.3055e-04, -7.7746e-04, -1.4480e-04,\n",
      "        -6.5150e-04,  4.1113e-04, -7.8266e-04, -8.8875e-05, -2.6195e-05,\n",
      "         5.1403e-04,  1.8850e-04,  5.4239e-04,  3.1416e-04, -8.2211e-04,\n",
      "        -2.3932e-03])\n",
      "OUTPUT will be saved at: /tigress/cw55/work/2022_radi_nn/NN_lw_test/AM4_v0/afclr_L5_EN_A100/restart.04.pth\n",
      "Train info >> run: 4 lr_sta: 1.0e-04, batch size: 64000\n",
      "Epoch 000001 |train loss: 8.23e-04 4.19e-01 | test loss: 4.07e-02 6.79e-01  |  use   0s | eta   0 min\n",
      "Epoch 000501 |train loss: 1.22e-03 5.12e-01 | test loss: 1.67e-03 6.74e-01  |  use   8s | eta   1 min\n",
      "Epoch 001001 |train loss: 1.13e-03 5.30e-01 | test loss: 1.49e-03 4.79e-01  |  use   8s | eta   1 min\n",
      "Epoch 001501 |train loss: 8.81e-04 4.58e-01 | test loss: 1.42e-03 4.69e-01  |  use   8s | eta   0 min\n",
      "Epoch 002001 |train loss: 1.23e-03 4.69e-01 | test loss: 1.49e-03 4.94e-01  |  use   8s | eta   0 min\n",
      "Epoch 02289: reducing learning rate of group 0 to 2.0000e-05.\n",
      "Epoch 002501 |train loss: 1.23e-03 4.55e-01 | test loss: 1.06e-03 5.12e-01  |  use   8s | eta   0 min\n",
      "Epoch 003001 |train loss: 9.01e-04 4.34e-01 | test loss: 1.10e-03 4.75e-01  |  use   8s | eta   0 min\n",
      "Epoch 003501 |train loss: 8.13e-04 4.21e-01 | test loss: 1.06e-03 4.57e-01  |  use   8s | eta   0 min\n",
      "Epoch 03790: reducing learning rate of group 0 to 4.0000e-06.\n",
      "Epoch 004001 |train loss: 6.89e-04 4.72e-01 | test loss: 1.03e-03 4.78e-01  |  use   8s | eta   0 min\n",
      "Epoch 004501 |train loss: 7.57e-04 4.08e-01 | test loss: 1.03e-03 4.68e-01  |  use   8s | eta   0 min\n",
      "afclr_L5_EN_A100 Finished: run 5!\n",
      "TEST: RMSE, MAE, Bias\n",
      "tensor([0.6596, 0.5937, 0.3535, 0.0161, 0.0083, 0.0073, 0.0052, 0.0036, 0.0039,\n",
      "        0.0034, 0.0037, 0.0042, 0.0034, 0.0028, 0.0051, 0.0100, 0.0143, 0.0168,\n",
      "        0.0195, 0.0224, 0.0274, 0.0260, 0.0261, 0.0269, 0.0297, 0.0275, 0.0290,\n",
      "        0.0387, 0.0376, 0.0391, 0.0398, 0.0466, 0.0547, 0.0712, 0.1019, 0.1600]) tensor([0.4706, 0.4474, 0.2580, 0.0114, 0.0052, 0.0050, 0.0035, 0.0026, 0.0025,\n",
      "        0.0023, 0.0027, 0.0031, 0.0025, 0.0020, 0.0036, 0.0074, 0.0103, 0.0124,\n",
      "        0.0138, 0.0162, 0.0198, 0.0181, 0.0180, 0.0184, 0.0199, 0.0180, 0.0179,\n",
      "        0.0208, 0.0210, 0.0213, 0.0216, 0.0238, 0.0273, 0.0340, 0.0501, 0.0946]) tensor([ 0.0991, -0.0059,  0.0297, -0.0028,  0.0005, -0.0013, -0.0005, -0.0002,\n",
      "        -0.0007, -0.0001,  0.0004,  0.0011,  0.0008,  0.0002, -0.0009, -0.0016,\n",
      "         0.0014,  0.0003, -0.0026, -0.0027, -0.0026, -0.0009,  0.0005,  0.0012,\n",
      "        -0.0015, -0.0015, -0.0006, -0.0013, -0.0012, -0.0040, -0.0004, -0.0016,\n",
      "        -0.0023, -0.0021, -0.0054, -0.0162])\n",
      "All runs finished. Increase <run_num> if you need to continue to train the model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import xarray as xr\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    torch.cuda.set_device(0) # select gpu_id, default 0 means the first GPU\n",
    "    device = f'cuda:{torch.cuda.current_device()}'\n",
    "    ######################################################\n",
    "    # set exp name and runs\n",
    "    Exp_name = 'afclr_L5_EN_A100' \n",
    "    work_dir = '/tigress/cw55/work/2022_radi_nn/NN_AM4/work/'\n",
    "    total_run_num  = 5\n",
    "    epochs = 5000\n",
    "    de_save = 500\n",
    "    \n",
    "    ######################################################\n",
    "    # create dir for first run\n",
    "    run_num, exp_dir = return_exp_dir(work_dir, Exp_name)\n",
    "    # copy script to experiment dir for reference\n",
    "    try:\n",
    "        ossyscmd = f'cp {os.path.abspath(__file__)} {exp_dir}/train_script.{run_num:02d}' \n",
    "        os.system(ossyscmd) \n",
    "    except: pass\n",
    "    ######################################################\n",
    "    \n",
    "    # get data and do normalization\n",
    "    if run_num == 0:  \n",
    "        nor_para = None\n",
    "        model_state_dict = None\n",
    "        lr_sta = 1e-3\n",
    "    else:  \n",
    "        PATH_last =  exp_dir+f'/restart.{run_num-1:02d}.pth'\n",
    "        restart_data = torch.load(PATH_last)  # load exist results and restart training\n",
    "        print(f'restart from: {PATH_last}')\n",
    "        # read training dataset, nor_para, model parameteres\n",
    "        nor_para = restart_data['nor_para']\n",
    "        model_state_dict = restart_data['model_state_dict']\n",
    "        lr_sta = 1e-4\n",
    "    data_date_info = '/tigress/cw55/data/NNRTMC_dataset/AM4_v0_in_out/lw_csaf.1018.p'\n",
    "    data_info, input_array_ori, output_array_ori = \\\n",
    "    pickle.load(open(data_date_info,'rb'))\n",
    "    \n",
    "    hybrid_p_sigma_para = xr.open_dataset('/tigress/cw55/data/NNRTMC_dataset/AM4_pk_bk_202207.nc')\n",
    "    A_k = hybrid_p_sigma_para.ak.values[None,:]\n",
    "    B_k = hybrid_p_sigma_para.bk.values[None,:]\n",
    "    \n",
    "    # remove dependent data, like upper level pressure\n",
    "    input_array  = input_array_ori [:100000,33:]\n",
    "    output_array = output_array_ori[:100000,:]\n",
    "    \n",
    "    nor_para, input_array, output_array   = data_std_normalization(input_array, output_array, nor_para)\n",
    " \n",
    "    # move all data to GPU to accelerate training\n",
    "    input_torch = torch.tensor(input_array, dtype=torch.float32).to(device)\n",
    "    output_torch = torch.tensor(output_array, dtype=torch.float32).to(device) \n",
    "     \n",
    "    # set random generator\n",
    "    # rng = np.random.default_rng(12345)\n",
    "    rng = np.random.default_rng()\n",
    "    # divide the training and test data here\n",
    "    # this could be different if restart the training process\n",
    "    ind_train, ind_test = split_train_test_sample(output_array.shape[0], test_ratio=0.3, rng=rng) \n",
    "    \n",
    "    # initialize model\n",
    "    NNRTMC_solver = ResNNRTMC(device, nor_para, A_k, B_k, model_state_dict)  \n",
    "    \n",
    "    ######################################################\n",
    "    # training for n times\n",
    "    for i in range(run_num, total_run_num): \n",
    "        PATH =  exp_dir+f'/restart.{i:02d}.pth'\n",
    "        print('OUTPUT will be saved at: '+PATH) \n",
    "        loss = []\n",
    "        batch_size = max(2000, 4000*i**2)\n",
    "        print(f'Train info >> run: {i} lr_sta: {lr_sta:7.1e}, batch size: {batch_size}')\n",
    "        custom_trainning(NNRTMC_solver, lr_sta, loss, epochs, batch_size, de_save,\\\n",
    "                         input_torch, output_torch, \\\n",
    "                         ind_train, ind_test,  device, rng )\n",
    "        print(f'{Exp_name} Finished: run {i+1}!')  \n",
    "        loss_array = np.array(loss).squeeze().T  \n",
    "        ######################################################\n",
    "        # save model state dict and data normalization info\n",
    "        NNRTMC_solver.save_model_restart(PATH, loss_array, data_date_info, nor_para)\n",
    "        print_key_results( NNRTMC_solver, input_torch[ind_test,:], output_array[ind_test,:], nor_para)\n",
    "        lr_sta = 1e-4\n",
    "        \n",
    "    print('All runs finished. Increase <run_num> if you need to continue to train the model.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
